# Custom_Models_NLP
Various Methodologies to create custom methodologies to generate and train ML extraction techniques

# Symptom and Additional Data Extraction with NLP and BERT

This repository implements two distinct approaches for extracting clinical information (symptoms, medications, locations, age, gender, and symptom duration) from natural language text.

---

## 1. Hybrid Pipeline Using spaCy, Custom NER, and SBERT Validation

### Overview

This pipeline combines three complementary modules:

- **Custom NER Training (via spaCy):**  
  We start with a base spaCy model (`en_core_web_sm`) and fine‑tune its Named Entity Recognizer (NER) using training data generated by GPT‑3.5. The training data is generated *intuitively* (without fixed templates) and contains natural sentences with entity annotations for labels such as `SYMPTOM`, `MEDICATION`, `LOCATION`, `AGE`, `GENDER`, and `DURATION`.

- **Inherent Rule‑based Extraction:**  
  In addition to the custom NER model, we use pre‑defined fixed lists—such as a canonical symptom list with corresponding synonyms, a list of medications, locations (e.g. Indian cities), etc.—to extract entities by simple matching (with minimal negation handling).

- **SBERT Semantic Validation:**  
  To overcome cases where different phrasings carry the same meaning (for example, “high temperature” vs. “fever”), we use SentenceTransformer (SBERT) to compute semantic embeddings and perform cosine similarity between candidate phrases and a reference index (built from the canonical symptom list and its synonyms). This “fuzzy matching” helps ensure that synonyms are correctly mapped.

### Key Components and Code Files

- **`generate_json_gpt.py`:**  
  - **Purpose:** Generate 150 training examples for spaCy NER using GPT‑3.5.  
  - **Details:** Defines all variable lists (symptom_list, symptom_synonyms, medications_list, location_list, gender_list, age_terms, and duration_list) in one file; randomly selects (for each example) a canonical symptom (with a possibility of replacing it with one of its synonyms); builds a GPT‑3.5 prompt that instructs the model to output a JSON array containing a natural sentence in context and entity annotations (with non‑overlapping spans).  
  - **Output:** Saves the examples to `gpt_training_data.json`.

- **`train_ner.py`:**  
  - **Purpose:** Fine‑tune the base spaCy model using the GPT‑generated training data.  
  - **Details:**  
    - Loads the JSON training examples and filters out examples with overlapping entity spans.  
    - Creates spaCy `Example` objects (using spaCy v3’s `Example.from_dict()` API).  
    - Fine‑tunes the NER component by adding the new labels (`SYMPTOM`, `MEDICATION`, `LOCATION`, `AGE`, `GENDER`, `DURATION`) and training over multiple iterations.  
  - **Output:** Saves the fine‑tuned model to the directory `./custom_model`.

- **`extract.py`:**  
  - **Purpose:** Extract target entities from new input text by combining:  
    - Custom model predictions (using `nlp_custom` loaded from the fine‑tuned model).  
    - Inherent rule‑based matching (using a simple spaCy model and fixed lists for symptoms, medications, and locations).  
    - SBERT-based semantic validation (using a pre‑trained SBERT model for fuzzy matching of symptom synonyms).  
  - **Details:** Merges the results from the custom NER, fixed-list matching, and SBERT similarity matching to improve recall and precision.  
  - **Output:** Returns a dictionary of extracted entities.

**Issues:**  
- Accuracy is not always optimal for some symptom synonyms.  
- The combined pipeline may be slow (up to ~8 seconds per input).

---

## 2. SBERT‑Only Pipeline for Symptom Extraction with Custom Fine‑Tuning

### Overview

In the second approach, we dispense with spaCy entirely and use SentenceTransformer (SBERT) as the sole backbone for symptom extraction. Since SBERT is a semantic encoder rather than an entity extractor, our strategy is two‑fold:

- **Fine‑Tuning SBERT on Domain Data:**  
  We create a custom dataset (e.g. `sbert_symptom_data.json`) containing pairs of (sentence, canonical symptom). This dataset is generated *intuitively* using GPT‑3.5 so that each sample includes a natural sentence and its corresponding canonical label (for example, mapping “high temperature” to “fever”). We fine‑tune an SBERT model (using contrastive learning with a loss such as MultipleNegativesRankingLoss) so that embeddings of sentences expressing the same symptom become similar.

- **Inference via Semantic Similarity:**  
  At inference time, the fine‑tuned SBERT model encodes input text (typically split into sentences). We also build a reference index by encoding candidate symptom phrases from our `symptom_list` (and synonyms). By computing cosine similarity between the sentence embeddings and the reference embeddings, we determine which canonical symptom is most likely expressed.

### Key Components and Code Files

- **Data Generation for SBERT:**  
  - A GPT‑3.5 based script (e.g. `generate_sbert_symptom_data.py`) generates 150 training samples.  
  - **Output:** Each sample is a JSON object with keys `"sentence"` and `"symptom"`, and the data is saved to `sbert_symptom_data.json`.

- **Fine‑Tuning Script (`train_sbert_symptoms.py`):**  
  - **Purpose:** Fine‑tune a SentenceTransformer model using the custom dataset.  
  - **Details:** Each training sample is treated as a pair (sentence, canonical symptom); the model is fine‑tuned using MultipleNegativesRankingLoss over several epochs.  
  - **Output:** Saves the fine‑tuned model as `"custom_sbert_model"`.

- **Inference Script (`sbert_inference.py`):**  
  - **Purpose:** Use the fine‑tuned SBERT model for symptom extraction.  
  - **Details:**  
    - Loads `"custom_sbert_model"` and constructs a reference index by encoding candidate symptom phrases (from `symptom_list` and `symptom_synonyms`).
    - Splits input text into sentences and computes embeddings.
    - For each sentence, computes cosine similarity with the reference embeddings and, if the similarity exceeds a threshold, returns the best-matched canonical symptom.  
  - **Output:** Returns the extracted symptom (if any) with similarity scores.

**Issues:**  
- Inference with the SBERT‑only pipeline may be slow (up to ~5 seconds per input).

---

## Summary

### Hybrid Pipeline (spaCy + SBERT):
- **Pros:** Combines statistical NER from spaCy, rule‑based matching, and semantic validation; handles multiple entity types (SYMPTOM, MEDICATION, LOCATION, AGE, GENDER, DURATION).
- **Cons:** Complex architecture and relatively slow processing (up to 8 seconds per input); occasional inaccuracies with symptom synonyms.

### SBERT‑Only Pipeline:
- **Pros:** Simplified architecture using only semantic embeddings; fine‑tuning improves domain-specific performance.
- **Cons:** SBERT is not a true entity extractor and requires careful candidate span extraction; inference speeds may be around 5 seconds per input.

Each approach relies on custom training data generated intuitively by GPT‑3.5. The hybrid method fuses multiple strategies for robust extraction, while the SBERT‑only method leverages fine‑tuned embeddings for semantic matching.

---

## How to Use

### For the Hybrid Pipeline:
1. **Generate Training Data:**  
   Run `generate_json_gpt.py` to produce 150 training examples (saved as `gpt_training_data.json`).
2. **Train Custom NER:**  
   Run `train_ner.py` to fine‑tune the spaCy model using the generated data. The model is saved to `./custom_model`.
3. **Extract Entities:**  
   Run `extract.py` to extract target entities from new text using the combined approach.

### For the SBERT‑Only Pipeline:
1. **Generate Fine‑Tuning Data:**  
   Run `generate_sbert_symptom_data.py` to produce 150 training samples (saved as `sbert_symptom_data.json`).
2. **Fine‑Tune SBERT:**  
   Run `train_sbert_symptoms.py` to fine‑tune the SBERT model using your custom data. The model is saved as `"custom_sbert_model"`.
3. **Perform Inference:**  
   Run `sbert_inference.py` to extract symptoms from new text based on semantic similarity.

---

## Issues and Considerations

- **Accuracy:**  
  Fine‑tuning and robust candidate matching are essential for correctly recognizing symptom synonyms.
- **Speed:**  
  The hybrid pipeline can be slow (up to ~8 seconds per input) while the SBERT‑only pipeline may take up to ~5 seconds per input.
- **Future Improvements:**  
  - Refine negation handling and improve candidate span extraction.
  - Expand and update fixed lists for better coverage.
  - Optimize SBERT inference (e.g., using Faiss for faster similarity search).

---

This README provides an in‑depth technical overview and usage instructions for our approaches to symptom and additional data extraction. Please refer to the individual code files for implementation details.
